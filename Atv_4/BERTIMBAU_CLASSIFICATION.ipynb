{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset (substituir pelo seu dataset se necessário)\n",
    "dataset = load_dataset(\"??\")  # Insira o nome correto da base em português\n",
    "\n",
    "# Divisão entre treino (70%), validação (10%) e teste (20%)\n",
    "train_test_split = dataset[\"train\"].train_test_split(test_size=0.3)\n",
    "test_valid_split = train_test_split[\"test\"].train_test_split(test_size=2/3)\n",
    "\n",
    "dataset_final = DatasetDict({\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"validation\": test_valid_split[\"train\"],\n",
    "    \"test\": test_valid_split[\"test\"]\n",
    "})\n",
    "\n",
    "# Exibir número de exemplos por divisão\n",
    "for split in dataset_final:\n",
    "    print(f\"{split}: {len(dataset_final[split])} exemplos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o tokenizer do BERTimbau\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "# Função para tokenizar\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Aplicar tokenização\n",
    "tokenized_datasets = dataset_final.map(tokenize_function, batched=True)\n",
    "\n",
    "# Visualizar tokens de um exemplo\n",
    "example_text = dataset_final[\"train\"][0][\"text\"]\n",
    "tokens = tokenizer.tokenize(example_text)\n",
    "print(\"Tokens de um documento:\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo BERTimbau pré-treinado para classificação\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", num_labels=2)\n",
    "\n",
    "# Configurar os parâmetros de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_bertimbau\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs_bertimbau\"\n",
    ")\n",
    "\n",
    "# Função de avaliação com métricas\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1_micro = f1_score(labels, preds, average=\"micro\")\n",
    "    f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"f1_micro\": f1_micro, \"f1_macro\": f1_macro}\n",
    "\n",
    "# Criar Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Iniciar o treinamento\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer previsões\n",
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Calcular métricas principais\n",
    "acc = accuracy_score(labels, preds)\n",
    "f1_micro = f1_score(labels, preds, average=\"micro\")\n",
    "f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "conf_matrix = confusion_matrix(labels, preds)\n",
    "\n",
    "# Exibir métricas principais\n",
    "print(f\"Acurácia: {acc:.4f}\")\n",
    "print(f\"F1-score (Micro): {f1_micro:.4f}\")\n",
    "print(f\"F1-score (Macro): {f1_macro:.4f}\")\n",
    "\n",
    "# Exibir matriz de confusão\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predito\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de Confusão\")\n",
    "plt.show()\n",
    "\n",
    "# **Relatório detalhado das métricas**\n",
    "print(\"Relatório de Classificação:\\n\")\n",
    "print(classification_report(labels, preds, target_names=[\"Negativo\", \"Positivo\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
