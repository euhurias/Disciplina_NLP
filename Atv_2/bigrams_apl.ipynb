{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando as bibliotecas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/hurias/Documentos/Disciplina_NLP/Atv_1')\n",
    "\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import io\n",
    "from base import get_stats, merge\n",
    "from BPE import BPE_Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparação dos dados\n",
    "\n",
    "Etapa é feito a tokenização o corpus e dividir os dados em treino e teste (80% e 20%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos lidos: 10000\n"
     ]
    }
   ],
   "source": [
    "pasta_json = r\"/home/hurias/Downloads/corpus\" \n",
    "count = 0\n",
    "\n",
    "merge_texto = io.StringIO()\n",
    "\n",
    "# Carregar os arquivos JSON e extrair os textos\n",
    "for filename in os.listdir(pasta_json):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(pasta_json, filename)\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "                count += 1\n",
    "                if 'text' in data:\n",
    "                    merge_texto.write(data['text'] + \" \")\n",
    "                else:\n",
    "                    print(f\"A chave 'text' não encontrada no arquivo: {filename}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Erro ao decodificar o JSON no arquivo: {filename}\")\n",
    "\n",
    "print(f\"Arquivos lidos: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do corpus: 71033261 caracteres\n"
     ]
    }
   ],
   "source": [
    "# Obtenha o texto combinado do corpus\n",
    "corpus = merge_texto.getvalue()\n",
    "print(f\"Tamanho do corpus: {len(corpus)} caracteres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifique se o corpus não está vazio\n",
    "if len(corpus.strip()) == 0:\n",
    "    print(\"Erro: O corpus está vazio!\")\n",
    "    sys.exit(1)  # Encerra a execução se o corpus estiver vazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar o BPE Tokenizer\n",
    "num_merges = 100\n",
    "tokenizer = BPE_Tokenizer(num_merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do texto em caracteres: 71033261\n",
      "Tamanho do texto em tokens: 73037124\n",
      "Tamanho da de tokens após BPE: 44430539\n",
      "Taxa de compressão: 1.64X\n",
      "Vocabulário após treinamento: {0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b'o ', 257: b'a ', 258: b'e ', 259: b's ', 260: b', ', 261: b'de ', 262: b'en', 263: b'm ', 264: b'or', 265: b'er', 266: b'an', 267: b'ar', 268: b'es', 269: b'co', 270: b'. ', 271: b'do ', 272: b'os ', 273: b'in', 274: b'al', 275: b'as ', 276: b'\\xc3\\xa3', 277: b'ad', 278: b'ent', 279: b'\\xc3\\xa3o ', 280: b'\\xc3\\xa7', 281: b'ri', 282: b'ci', 283: b're', 284: b'qu', 285: b'st', 286: b'at', 287: b'\\xc3\\xa9', 288: b'on', 289: b'el', 290: b'es ', 291: b'da ', 292: b'ic', 293: b'em ', 294: b'as', 295: b'it', 296: b'am', 297: b'\\xc3\\xad', 298: b'ro', 299: b'\\xc3\\xa1', 300: b'u ', 301: b'| ', 302: b'==', 303: b'di', 304: b'ai', 305: b'ei', 306: b'a\\xc3\\xa7', 307: b'id', 308: b'que ', 309: b'os', 310: b'em', 311: b'il', 312: b'un', 313: b'est', 314: b'ul', 315: b'con', 316: b'19', 317: b'par', 318: b'or ', 319: b'um', 320: b'a, ', 321: b'al ', 322: b'ol', 323: b'o de ', 324: b'ant', 325: b'o, ', 326: b'ur', 327: b'* ', 328: b'|| ', 329: b'\\xc3\\xb3', 330: b'com', 331: b'a de ', 332: b'im', 333: b'et', 334: b'20', 335: b'ut', 336: b'ist', 337: b'eg', 338: b'ado ', 339: b'a\\xc3\\xa7\\xc3\\xa3o ', 340: b'po', 341: b'se ', 342: b'um ', 343: b'ar ', 344: b'ra', 345: b'iv', 346: b'\\xc3\\xa9 ', 347: b'om', 348: b'ia ', 349: b'com ', 350: b'ou ', 351: b'is', 352: b'i ', 353: b'eir', 354: b'am ', 355: b'uma '}\n"
     ]
    }
   ],
   "source": [
    "# Verifique o vocabulário após o treinamento\n",
    "tokenizer.train(corpus)\n",
    "print(\"Vocabulário após treinamento:\", tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifique se o vocabulário foi gerado corretamente\n",
    "if len(tokenizer.vocab) == 0:\n",
    "    print(\"Erro: O vocabulário não foi gerado corretamente.\")\n",
    "    sys.exit(1)  # Encerra a execução se o vocabulário não foi gerado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use o tokenizador para segmentar o corpus\n",
    "tokens = tokenizer.encode(text=corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento do corpus e Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de tokens no treino: 35544431\n",
      "Total de tokens no teste: 8886108\n"
     ]
    }
   ],
   "source": [
    "# Divisão em treino (80%) e teste (20%)\n",
    "split_index = int(len(tokens) * 0.8)\n",
    "random.shuffle(tokens)\n",
    "train_tokens = tokens[:split_index]\n",
    "test_tokens = tokens[split_index:]\n",
    "\n",
    "print(\"Total de tokens no treino:\", len(train_tokens))\n",
    "print(\"Total de tokens no teste:\", len(test_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigram import BigramModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treine o modelo bigrama\n",
    "\n",
    "bigram_model = BigramModel()\n",
    "bigram_model.train(train_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity do modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexidade do modelo: 124.84411359166064\n"
     ]
    }
   ],
   "source": [
    "# Função para calcular a perplexidade\n",
    "def calculate_perplexity(model, tokens):\n",
    "    perplexity = 0\n",
    "    N = len(tokens)\n",
    "    for i in range(len(tokens) - 1):\n",
    "        prob = model.bigram_prob(tokens[i], tokens[i + 1])\n",
    "        if prob > 0:\n",
    "            perplexity += -math.log(prob)\n",
    "    perplexity = math.exp(perplexity / N)\n",
    "    return perplexity\n",
    "\n",
    "# Calcule a perplexidade no conjunto de teste\n",
    "perplexity = calculate_perplexity(bigram_model, test_tokens)\n",
    "print(\"Perplexidade do modelo:\", perplexity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geração de Texto \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para gerar texto a partir de um token inicial\n",
    "def generate_text(model, last_token, length=20):\n",
    "    text = [last_token]\n",
    "    \n",
    "    for _ in range(length - 1):\n",
    "        # Obtenha as próximas palavras e suas probabilidades condicionais\n",
    "        next_words = list(model.bigram_counts[last_token].keys())\n",
    "        next_probs = [model.bigram_prob(last_token, word) for word in next_words]\n",
    "        \n",
    "        # Verificar se os próximos tokens estão no vocabulário\n",
    "        next_words = [word for word in next_words if word in tokenizer.vocab]\n",
    "        if not next_words: \n",
    "            break \n",
    "        \n",
    "        # Normaliza as probabilidades\n",
    "        next_probs = np.array(next_probs)\n",
    "        next_probs /= next_probs.sum()\n",
    "\n",
    "        # Seleciona a próxima palavra com base na probabilidade condicional\n",
    "        last_token = np.random.choice(next_words, p=next_probs)\n",
    "        text.append(last_token)\n",
    "    \n",
    "    # Decodificar os tokens de volta para palavras (strings)\n",
    "    decoded_text = [tokenizer.decode([token]) for token in text]\n",
    "    \n",
    "    # Juntar as palavras para formar uma string de texto gerado\n",
    "    return \" \".join(decoded_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fragmento de texto: Flamengo é o melhor time \n",
      "Tokens para 'Flamengo': [70, 108, 296, 262, 103, 111]\n",
      "Erro ao codificar a palavra 'é': min() iterable argument is empty\n",
      "Erro ao codificar a palavra 'o': min() iterable argument is empty\n",
      "Tokens para 'melhor': [109, 289, 104, 264]\n",
      "Tokens para 'time': [116, 332, 101]\n",
      "Último token: 101\n",
      "Texto gerado: e que  st   do  v st b e  í de  c est d s  o  u t ur r\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de como gerar texto a partir do fragmento\n",
    "text_frag = \"Flamengo é o melhor time \"\n",
    "print(\"Fragmento de texto:\", text_frag)\n",
    "\n",
    "# Tente codificar as palavras individualmente\n",
    "frag_tokens = []\n",
    "for word in text_frag.split():\n",
    "    try:\n",
    "        tokens = tokenizer.encode(word)\n",
    "        print(f\"Tokens para '{word}': {tokens}\")\n",
    "        frag_tokens.extend(tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao codificar a palavra '{word}': {e}\")\n",
    "\n",
    "# Se houver tokens, pegue o último\n",
    "if frag_tokens:\n",
    "    last_token = frag_tokens[-1]\n",
    "    print(\"Último token:\", last_token)\n",
    "else:\n",
    "    print(\"Nenhum token foi gerado.\")\n",
    "\n",
    "# Gere texto a partir do último token\n",
    "generated_text = generate_text(bigram_model, last_token)\n",
    "print(\"Texto gerado:\", generated_text)  # Agora isso deve gerar o texto legível\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
